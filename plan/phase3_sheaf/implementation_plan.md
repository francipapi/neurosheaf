
com# Phase 3: Sheaf Construction Implementation Plan (Weeks 5-7)

## Overview
Implement the sheaf construction module with FX-based automatic poset extraction, scaled Procrustes restriction maps, and optimized sparse Laplacian assembly.

## ðŸŽ¯ CRITICAL DESIGN UPDATE (Week 6 Complete)

**BREAKTHROUGH**: Pure whitened coordinate implementation achieved **100% acceptance criteria success**.

### Key Changes
- **Mandatory whitening**: All sheaf operations occur in whitened coordinate space only
- **No back-transformation**: Whitened space is the natural coordinate system for sheaves  
- **Exact properties**: Machine precision accuracy (errors < 1e-12) for core mathematical axioms
- **Production ready**: Maintains <3GB memory and <5min runtime targets

### Implementation Status
- âœ… **Week 6 COMPLETED**: WhiteningProcessor, ProcrustesMaps, SheafBuilder implemented
- âœ… **Acceptance criteria**: 100% success rate with pure whitened approach
- âœ… **Mathematical validation**: All sheaf axioms satisfied exactly
- ðŸŽ¯ **Next**: Phase 4 spectral analysis using whitened Laplacian

## Week 5: FX-based Poset Extraction

### Day 1-2: FX Graph Analysis Foundation
**Reference**: docs/comprehensive-implementation-plan-v3.md - "FX-based Generic Poset Extraction"
- [ ] Implement torch.fx symbolic tracing wrapper
- [ ] Create graph traversal utilities
- [ ] Build node dependency analyzer
- [ ] Handle special operations (reshape, view, etc.)
- [ ] Create fallback for dynamic models

### Day 3-4: Poset Construction from FX Graph
- [ ] Implement topological ordering of layers
- [ ] Detect skip connections automatically
- [ ] Build partial order relationships
- [ ] Handle parallel branches (inception-style)
- [ ] Create poset visualization utilities

### Day 5: Architecture-Specific Handlers
- [ ] ResNet skip connection detection
- [ ] Transformer attention pattern recognition
- [ ] CNN pooling hierarchy extraction
- [ ] RNN/LSTM state flow tracking
- [ ] Custom module registration system

## Week 6: Restriction Maps and Sheaf Structure âœ… **COMPLETED**

### Day 6-7: Scaled Procrustes Implementation âœ… **COMPLETED**
**Reference**: docs/updated-sheaf-construction-v3.md - "Scaled Procrustes restriction maps"
- [x] âœ… Implement Procrustes alignment algorithm (with whitening support)
- [x] âœ… Add scaling factor optimization  
- [x] âœ… Create dimension mismatch handlers
- [x] âœ… Implement orthogonal projection fallback
- [x] âœ… Add numerical stability checks
- [x] âœ… **BREAKTHROUGH**: WhiteningProcessor for exact metric compatibility

### Day 8-9: Sheaf Construction âœ… **COMPLETED**
- [x] âœ… Define Sheaf data structure (with whitened stalks)
- [x] âœ… Implement stalk assignment from activations
- [x] âœ… Create restriction map computation (pure whitened coordinates)
- [x] âœ… Build sheaf morphism validators (exact properties)
- [x] âœ… Add consistency checking (100% acceptance criteria)

### Day 10: Integration with CKA âœ… **COMPLETED**
- [x] âœ… Connect CKA matrices as stalk data (uncentered Gram matrices)
- [x] âœ… Implement sheaf section spaces (whitened coordinates)
- [x] âœ… Create global section extractors
- [x] âœ… Add sheaf cohomology utilities (exact transitivity)
- [x] âœ… Validate mathematical properties (machine precision accuracy)

## Week 7: Laplacian Assembly and Optimization

### Day 11-12: Sparse Laplacian Construction
**Reference**: docs/updated-optimized-laplacian-persistence-v3.md - "Optimized sparse Laplacian assembly"
- [ ] Implement sparse matrix builders
- [ ] Create block-diagonal structure
- [ ] Add boundary operators
- [ ] Optimize memory layout
- [ ] Implement fast matrix-vector products

### Day 13-14: Performance Optimization
- [ ] GPU sparse operations
- [ ] Memory pooling for large networks
- [ ] Lazy evaluation strategies
- [ ] Cache intermediate computations
- [ ] Parallel Laplacian assembly

### Day 15: Validation and Testing
- [ ] Mathematical correctness tests
- [ ] Performance benchmarks
- [ ] Architecture coverage tests
- [ ] Memory usage validation
- [ ] Integration test suite

## Implementation Details

### FX-based Poset Extraction
```python
# neurosheaf/sheaf/poset.py
import torch
import torch.fx as fx
from typing import Dict, List, Set, Tuple, Optional
import networkx as nx
from ..utils.logging import setup_logger
from ..utils.exceptions import ArchitectureError

logger = setup_logger(__name__)

class FXPosetExtractor:
    """Extract poset structure from PyTorch models using FX."""
    
    def __init__(self, handle_dynamic: bool = True):
        self.handle_dynamic = handle_dynamic
        self._module_index = {}
        
    def extract_poset(self, model: torch.nn.Module) -> nx.DiGraph:
        """Extract poset from model using FX symbolic tracing.
        
        Returns:
            NetworkX directed graph representing the poset
        """
        try:
            # Symbolic trace the model
            traced = fx.symbolic_trace(model)
            return self._build_poset_from_graph(traced.graph)
        except Exception as e:
            if self.handle_dynamic:
                logger.warning(f"FX tracing failed: {e}. Falling back to module inspection.")
                return self._fallback_extraction(model)
            else:
                raise ArchitectureError(f"Cannot trace model: {e}")
    
    def _build_poset_from_graph(self, graph: fx.Graph) -> nx.DiGraph:
        """Build poset from FX graph."""
        poset = nx.DiGraph()
        
        # First pass: identify all nodes that produce activations
        activation_nodes = {}
        for node in graph.nodes:
            if self._is_activation_node(node):
                node_id = self._get_node_id(node)
                activation_nodes[node] = node_id
                poset.add_node(node_id, 
                             name=node.name,
                             op=node.op,
                             target=str(node.target))
        
        # Second pass: build edges based on data flow
        for node in graph.nodes:
            if node in activation_nodes:
                for user in node.users:
                    if user in activation_nodes:
                        # Direct connection
                        poset.add_edge(activation_nodes[node], 
                                     activation_nodes[user])
                    else:
                        # Check for skip connections through ops
                        for downstream in self._find_downstream_activations(user, activation_nodes):
                            poset.add_edge(activation_nodes[node], 
                                         activation_nodes[downstream])
        
        # Add layer indices for ordering
        self._add_topological_levels(poset)
        
        return poset
    
    def _is_activation_node(self, node: fx.Node) -> bool:
        """Check if node produces activations we care about."""
        # Skip certain operations
        skip_ops = {'placeholder', 'output', 'get_attr'}
        if node.op in skip_ops:
            return False
        
        # Include calls to modules and functions that transform features
        if node.op == 'call_module':
            return True
        
        if node.op == 'call_function':
            # Include operations that preserve feature structure
            preserve_ops = {
                torch.nn.functional.relu,
                torch.nn.functional.gelu,
                torch.add,
                torch.cat,
            }
            return node.target in preserve_ops
        
        return False
    
    def _find_downstream_activations(self, node: fx.Node, 
                                   activation_nodes: Dict) -> List[fx.Node]:
        """Find activation nodes downstream from given node."""
        downstream = []
        visited = set()
        
        def traverse(n):
            if n in visited or n in activation_nodes:
                if n in activation_nodes:
                    downstream.append(n)
                return
            visited.add(n)
            for user in n.users:
                traverse(user)
        
        traverse(node)
        return downstream
    
    def _add_topological_levels(self, poset: nx.DiGraph):
        """Add topological level information to nodes."""
        # Compute levels using longest path
        levels = nx.dag_longest_path_length(poset, weight=None)
        
        for node in nx.topological_sort(poset):
            level = 0
            for pred in poset.predecessors(node):
                level = max(level, poset.nodes[pred].get('level', 0) + 1)
            poset.nodes[node]['level'] = level
    
    def _fallback_extraction(self, model: torch.nn.Module) -> nx.DiGraph:
        """Fallback to module-based extraction for dynamic models."""
        poset = nx.DiGraph()
        
        # Extract modules and their connections
        modules = dict(model.named_modules())
        
        # Simple sequential assumption with skip detection
        prev_layers = []
        for name, module in modules.items():
            if self._is_feature_layer(module):
                poset.add_node(name, module=module)
                
                # Add edges from previous layers
                for prev in prev_layers[-3:]:  # Look back up to 3 layers
                    poset.add_edge(prev, name)
                
                prev_layers.append(name)
        
        return poset
```

### Scaled Procrustes Restriction Maps
```python
# neurosheaf/sheaf/restriction.py
import torch
import numpy as np
from typing import Tuple, Optional
from scipy.linalg import orthogonal_procrustes
from ..utils.exceptions import ComputationError

class ProcrustesMaps:
    """Compute restriction maps using scaled Procrustes analysis."""
    
    def __init__(self, 
                 allow_scaling: bool = True,
                 max_iter: int = 100,
                 tol: float = 1e-6):
        self.allow_scaling = allow_scaling
        self.max_iter = max_iter
        self.tol = tol
    
    def compute_restriction(self, 
                          source: torch.Tensor,
                          target: torch.Tensor,
                          method: str = 'scaled_procrustes') -> torch.Tensor:
        """Compute restriction map from source to target activations.
        
        Args:
            source: Activations from source layer [n_samples, d_source]
            target: Activations from target layer [n_samples, d_target]
            method: Method to use for alignment
            
        Returns:
            Restriction map matrix [d_source, d_target]
        """
        if method == 'scaled_procrustes':
            return self._scaled_procrustes(source, target)
        elif method == 'orthogonal':
            return self._orthogonal_projection(source, target)
        elif method == 'least_squares':
            return self._least_squares(source, target)
        else:
            raise ValueError(f"Unknown method: {method}")
    
    def _scaled_procrustes(self, 
                          source: torch.Tensor,
                          target: torch.Tensor) -> torch.Tensor:
        """Scaled Procrustes analysis."""
        # Center the data
        source_centered = source - source.mean(dim=0, keepdim=True)
        target_centered = target - target.mean(dim=0, keepdim=True)
        
        # Convert to numpy for scipy
        X = source_centered.detach().cpu().numpy()
        Y = target_centered.detach().cpu().numpy()
        
        # Standard Procrustes
        R, scale = orthogonal_procrustes(X, Y)
        
        if self.allow_scaling:
            # Compute optimal scaling
            scale = np.trace(Y.T @ X @ R) / np.trace(X.T @ X)
            scale = max(scale, 0.1)  # Prevent degenerate scaling
        else:
            scale = 1.0
        
        # Convert back to torch
        R_torch = torch.from_numpy(R).to(source.device).float()
        
        return scale * R_torch
    
    def _orthogonal_projection(self, 
                             source: torch.Tensor,
                             target: torch.Tensor) -> torch.Tensor:
        """Orthogonal projection when dimensions don't match."""
        d_source = source.shape[1]
        d_target = target.shape[1]
        
        if d_source == d_target:
            return torch.eye(d_source, device=source.device)
        
        # Compute correlation matrix
        C = source.T @ target / source.shape[0]
        
        # SVD for best rank-r approximation
        U, S, Vt = torch.linalg.svd(C, full_matrices=False)
        
        # Construct projection
        r = min(d_source, d_target)
        if d_source > d_target:
            # Project down
            return U[:, :r] @ Vt[:r, :]
        else:
            # Embed up
            proj = torch.zeros(d_source, d_target, device=source.device)
            proj[:, :d_source] = torch.eye(d_source, device=source.device)
            return proj
    
    def _least_squares(self, 
                      source: torch.Tensor,
                      target: torch.Tensor) -> torch.Tensor:
        """Simple least squares solution."""
        # Solve: min ||target - source @ R||_F
        R = torch.linalg.lstsq(source, target).solution
        return R
```

### Sheaf Construction
```python
# neurosheaf/sheaf/construction.py
import torch
import numpy as np
from typing import Dict, List, Optional, Tuple
import networkx as nx
from dataclasses import dataclass
from ..utils.logging import setup_logger
from .poset import FXPosetExtractor
from .restriction import ProcrustesMaps

logger = setup_logger(__name__)

@dataclass
class Sheaf:
    """Neural sheaf data structure."""
    poset: nx.DiGraph  # Underlying poset
    stalks: Dict[str, torch.Tensor]  # Stalk data at each node
    restrictions: Dict[Tuple[str, str], torch.Tensor]  # Restriction maps
    
    def validate(self) -> bool:
        """Validate sheaf axioms."""
        # Check restriction maps compose correctly
        for edge in self.poset.edges():
            if edge not in self.restrictions:
                logger.error(f"Missing restriction for edge {edge}")
                return False
        
        # Check transitivity
        for path in nx.all_simple_paths(self.poset, 
                                       source=self._get_sources(), 
                                       target=self._get_sinks(),
                                       cutoff=3):
            if len(path) >= 3:
                # Check R_ac = R_bc @ R_ab
                a, b, c = path[0], path[1], path[2]
                R_ab = self.restrictions.get((a, b))
                R_bc = self.restrictions.get((b, c))
                R_ac = self.restrictions.get((a, c))
                
                if R_ac is not None and R_ab is not None and R_bc is not None:
                    composed = R_bc @ R_ab
                    if not torch.allclose(composed, R_ac, atol=1e-4):
                        logger.warning(f"Restriction transitivity violated: {a}->{b}->{c}")
        
        return True
    
    def _get_sources(self) -> List[str]:
        """Get source nodes (no predecessors)."""
        return [n for n in self.poset.nodes() 
                if self.poset.in_degree(n) == 0]
    
    def _get_sinks(self) -> List[str]:
        """Get sink nodes (no successors)."""
        return [n for n in self.poset.nodes() 
                if self.poset.out_degree(n) == 0]


class SheafBuilder:
    """Build sheaves from neural network activations."""
    
    def __init__(self,
                 poset_extractor: Optional[FXPosetExtractor] = None,
                 restriction_computer: Optional[ProcrustesMaps] = None):
        self.poset_extractor = poset_extractor or FXPosetExtractor()
        self.restriction_computer = restriction_computer or ProcrustesMaps()
    
    def build_sheaf(self,
                   model: torch.nn.Module,
                   activations: Dict[str, torch.Tensor],
                   cka_matrices: Optional[Dict[str, torch.Tensor]] = None) -> Sheaf:
        """Construct sheaf from model and activations.
        
        Args:
            model: Neural network model
            activations: Dict mapping layer names to activation tensors
            cka_matrices: Optional pre-computed CKA matrices
            
        Returns:
            Constructed Sheaf object
        """
        # Extract poset structure
        logger.info("Extracting poset structure...")
        poset = self.poset_extractor.extract_poset(model)
        
        # Filter to only include layers we have activations for
        nodes_to_keep = [n for n in poset.nodes() if n in activations]
        poset = poset.subgraph(nodes_to_keep).copy()
        
        # Assign stalks
        logger.info("Assigning stalks...")
        stalks = {}
        for node in poset.nodes():
            if cka_matrices and node in cka_matrices:
                # Use CKA matrix as stalk data
                stalks[node] = cka_matrices[node]
            else:
                # Use activation statistics
                act = activations[node]
                stalk_data = self._compute_stalk_data(act)
                stalks[node] = stalk_data
        
        # Compute restriction maps
        logger.info("Computing restriction maps...")
        restrictions = {}
        for edge in poset.edges():
            source, target = edge
            source_act = activations[source]
            target_act = activations[target]
            
            R = self.restriction_computer.compute_restriction(
                source_act, target_act
            )
            restrictions[edge] = R
        
        # Add transitive restrictions for paths of length > 1
        self._add_transitive_restrictions(poset, activations, restrictions)
        
        # Create sheaf
        sheaf = Sheaf(poset=poset, stalks=stalks, restrictions=restrictions)
        
        # Validate
        if not sheaf.validate():
            logger.warning("Sheaf validation failed")
        
        return sheaf
    
    def _compute_stalk_data(self, activations: torch.Tensor) -> torch.Tensor:
        """Compute stalk data from activations."""
        # For now, return covariance matrix
        act_centered = activations - activations.mean(dim=0, keepdim=True)
        cov = act_centered.T @ act_centered / (activations.shape[0] - 1)
        return cov
    
    def _add_transitive_restrictions(self,
                                   poset: nx.DiGraph,
                                   activations: Dict[str, torch.Tensor],
                                   restrictions: Dict[Tuple[str, str], torch.Tensor]):
        """Add restriction maps for indirect connections."""
        # Find all pairs with path length > 1
        for source in poset.nodes():
            for target in poset.nodes():
                if source != target and (source, target) not in restrictions:
                    # Check if path exists
                    if nx.has_path(poset, source, target):
                        # Compute direct restriction
                        R = self.restriction_computer.compute_restriction(
                            activations[source],
                            activations[target]
                        )
                        restrictions[(source, target)] = R
```

### Sparse Laplacian Assembly
```python
# neurosheaf/sheaf/laplacian.py
import torch
import torch.sparse
import numpy as np
from typing import Dict, List, Optional, Tuple
from scipy.sparse import coo_matrix, block_diag
import networkx as nx
from .construction import Sheaf
from ..utils.memory import MemoryMonitor

class SparseLaplacianBuilder:
    """Build sparse sheaf Laplacians efficiently."""
    
    def __init__(self,
                 use_gpu: bool = True,
                 chunk_size: int = 1000):
        self.use_gpu = use_gpu and torch.cuda.is_available()
        self.chunk_size = chunk_size
        self.memory_monitor = MemoryMonitor()
    
    def build_laplacian(self, 
                       sheaf: Sheaf,
                       weight_edges: bool = True) -> torch.sparse.Tensor:
        """Build sparse sheaf Laplacian.
        
        Args:
            sheaf: Sheaf object
            weight_edges: Whether to weight edges by restriction quality
            
        Returns:
            Sparse Laplacian matrix
        """
        # Compute dimensions
        node_dims = {node: sheaf.stalks[node].shape[0] 
                    for node in sheaf.poset.nodes()}
        total_dim = sum(node_dims.values())
        
        # Node index mapping
        node_to_idx = {}
        current_idx = 0
        for node in sheaf.poset.nodes():
            node_to_idx[node] = (current_idx, current_idx + node_dims[node])
            current_idx += node_dims[node]
        
        # Build Laplacian in COO format
        rows, cols, values = [], [], []
        
        # Add diagonal blocks (degree terms)
        for node in sheaf.poset.nodes():
            start, end = node_to_idx[node]
            degree = sheaf.poset.degree(node)
            
            if degree > 0:
                # Add D_ii = degree * I
                for i in range(start, end):
                    rows.append(i)
                    cols.append(i)
                    values.append(float(degree))
        
        # Add off-diagonal blocks (restriction maps)
        for edge in sheaf.poset.edges():
            source, target = edge
            R = sheaf.restrictions[edge]
            
            source_start, source_end = node_to_idx[source]
            target_start, target_end = node_to_idx[target]
            
            # Weight by restriction quality if requested
            if weight_edges:
                weight = self._compute_edge_weight(R)
            else:
                weight = 1.0
            
            # Add -R and -R^T blocks
            R_weighted = -weight * R
            
            # Convert to COO format
            for i in range(R.shape[0]):
                for j in range(R.shape[1]):
                    if abs(R[i, j]) > 1e-8:  # Sparsity threshold
                        # -R block
                        rows.append(source_start + i)
                        cols.append(target_start + j)
                        values.append(R_weighted[i, j].item())
                        
                        # -R^T block
                        rows.append(target_start + j)
                        cols.append(source_start + i)
                        values.append(R_weighted[i, j].item())
        
        # Create sparse tensor
        indices = torch.LongTensor([rows, cols])
        values = torch.FloatTensor(values)
        
        L = torch.sparse_coo_tensor(
            indices, values, (total_dim, total_dim),
            device='cuda' if self.use_gpu else 'cpu'
        )
        
        return L
    
    def _compute_edge_weight(self, R: torch.Tensor) -> float:
        """Compute edge weight based on restriction quality."""
        # Use Frobenius norm of R as quality measure
        quality = torch.norm(R, 'fro')
        # Normalize to [0, 1]
        return float(torch.sigmoid(quality - 1))
    
    def build_normalized_laplacian(self, sheaf: Sheaf) -> torch.sparse.Tensor:
        """Build normalized sheaf Laplacian: L_norm = D^{-1/2} L D^{-1/2}."""
        L = self.build_laplacian(sheaf)
        
        # Compute degree matrix
        degrees = torch.sparse.sum(L, dim=1).to_dense()
        degrees = torch.clamp(degrees, min=1e-8)  # Avoid division by zero
        
        # D^{-1/2}
        d_inv_sqrt = torch.pow(degrees, -0.5)
        
        # Create diagonal matrix
        n = L.shape[0]
        D_inv_sqrt = torch.sparse_coo_tensor(
            torch.stack([torch.arange(n), torch.arange(n)]),
            d_inv_sqrt,
            (n, n),
            device=L.device
        )
        
        # Normalize: D^{-1/2} L D^{-1/2}
        L_norm = torch.sparse.mm(D_inv_sqrt, torch.sparse.mm(L, D_inv_sqrt))
        
        return L_norm
    
    def compute_fast_eigenvalues(self,
                               L: torch.sparse.Tensor,
                               k: int = 50,
                               method: str = 'lanczos') -> Tuple[torch.Tensor, torch.Tensor]:
        """Compute top-k eigenvalues of sparse Laplacian."""
        if method == 'lanczos':
            # Use sparse eigenvalue solver
            from scipy.sparse.linalg import eigsh
            
            # Convert to scipy sparse
            L_coo = L.coalesce()
            indices = L_coo.indices().cpu().numpy()
            values = L_coo.values().cpu().numpy()
            shape = L.shape
            
            L_scipy = coo_matrix(
                (values, (indices[0], indices[1])),
                shape=shape
            )
            
            # Compute smallest k eigenvalues
            eigenvalues, eigenvectors = eigsh(
                L_scipy, k=k, which='SM', tol=1e-6
            )
            
            # Convert back to torch
            eigenvalues = torch.from_numpy(eigenvalues).to(L.device)
            eigenvectors = torch.from_numpy(eigenvectors).to(L.device)
            
            return eigenvalues, eigenvectors
        else:
            raise ValueError(f"Unknown method: {method}")
```

## Testing Suite

### Test Structure
```
tests/phase3_sheaf/
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ test_fx_poset.py
â”‚   â”œâ”€â”€ test_procrustes.py
â”‚   â”œâ”€â”€ test_sheaf_construction.py
â”‚   â””â”€â”€ test_sparse_laplacian.py
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ test_architectures.py
â”‚   â””â”€â”€ test_sheaf_pipeline.py
â””â”€â”€ validation/
    â”œâ”€â”€ test_mathematical_properties.py
    â””â”€â”€ test_performance.py
```

### Critical Tests: FX Poset Extraction
```python
# tests/phase3_sheaf/unit/test_fx_poset.py
import pytest
import torch
import torch.nn as nn
import networkx as nx
from neurosheaf.sheaf.poset import FXPosetExtractor

class TestFXPosetExtraction:
    """Test automatic poset extraction using FX."""
    
    def test_simple_sequential(self):
        """Test extraction from simple sequential model."""
        model = nn.Sequential(
            nn.Linear(10, 20),
            nn.ReLU(),
            nn.Linear(20, 30),
            nn.ReLU(),
            nn.Linear(30, 10)
        )
        
        extractor = FXPosetExtractor()
        poset = extractor.extract_poset(model)
        
        # Should have linear structure
        assert len(poset.nodes()) == 5
        assert nx.is_directed_acyclic_graph(poset)
        
        # Check ordering
        topo_order = list(nx.topological_sort(poset))
        levels = [poset.nodes[n]['level'] for n in topo_order]
        assert levels == sorted(levels)
    
    def test_resnet_skip_connections(self):
        """Test extraction handles skip connections correctly."""
        class ResidualBlock(nn.Module):
            def __init__(self, dim):
                super().__init__()
                self.conv1 = nn.Conv2d(dim, dim, 3, padding=1)
                self.conv2 = nn.Conv2d(dim, dim, 3, padding=1)
                self.relu = nn.ReLU()
            
            def forward(self, x):
                identity = x
                out = self.relu(self.conv1(x))
                out = self.conv2(out)
                out = out + identity  # Skip connection
                return self.relu(out)
        
        model = ResidualBlock(64)
        extractor = FXPosetExtractor()
        poset = extractor.extract_poset(model)
        
        # Should detect skip connection
        # Find the add node
        add_nodes = [n for n in poset.nodes() 
                    if 'add' in poset.nodes[n].get('name', '')]
        assert len(add_nodes) > 0
        
        # Check it has multiple predecessors (skip connection)
        add_node = add_nodes[0]
        predecessors = list(poset.predecessors(add_node))
        assert len(predecessors) >= 2
    
    def test_dynamic_model_fallback(self):
        """Test fallback for dynamic models."""
        class DynamicModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.layers = nn.ModuleList([
                    nn.Linear(10, 20),
                    nn.Linear(20, 30),
                    nn.Linear(30, 10)
                ])
            
            def forward(self, x, n_layers=None):
                # Dynamic behavior
                n_layers = n_layers or len(self.layers)
                for i in range(n_layers):
                    x = self.layers[i](x)
                return x
        
        model = DynamicModel()
        extractor = FXPosetExtractor(handle_dynamic=True)
        
        # Should use fallback without crashing
        poset = extractor.extract_poset(model)
        assert len(poset.nodes()) > 0
    
    def test_parallel_branches(self):
        """Test extraction of inception-style parallel branches."""
        class InceptionBlock(nn.Module):
            def __init__(self, in_channels):
                super().__init__()
                self.branch1 = nn.Conv2d(in_channels, 64, 1)
                self.branch2 = nn.Sequential(
                    nn.Conv2d(in_channels, 48, 1),
                    nn.Conv2d(48, 64, 3, padding=1)
                )
                self.branch3 = nn.Sequential(
                    nn.Conv2d(in_channels, 64, 1),
                    nn.Conv2d(64, 96, 3, padding=1)
                )
            
            def forward(self, x):
                return torch.cat([
                    self.branch1(x),
                    self.branch2(x),
                    self.branch3(x)
                ], dim=1)
        
        model = InceptionBlock(256)
        extractor = FXPosetExtractor()
        poset = extractor.extract_poset(model)
        
        # Should have parallel structure
        # Find cat node
        cat_nodes = [n for n in poset.nodes()
                    if 'cat' in poset.nodes[n].get('name', '')]
        assert len(cat_nodes) > 0
        
        # Should have multiple predecessors (parallel branches)
        cat_node = cat_nodes[0]
        predecessors = list(poset.predecessors(cat_node))
        assert len(predecessors) >= 3
```

### Procrustes and Restriction Map Tests
```python
# tests/phase3_sheaf/unit/test_procrustes.py
import pytest
import torch
import numpy as np
from neurosheaf.sheaf.restriction import ProcrustesMaps

class TestProcrustesMaps:
    """Test Procrustes-based restriction maps."""
    
    def test_scaled_procrustes_alignment(self):
        """Test scaled Procrustes finds correct alignment."""
        # Create related data with known transformation
        n_samples = 100
        d = 50
        
        # True transformation: rotation + scaling
        theta = np.pi / 6
        R_true = torch.tensor([
            [np.cos(theta), -np.sin(theta)],
            [np.sin(theta), np.cos(theta)]
        ])
        scale_true = 1.5
        
        # Extend to higher dimensions
        R_full = torch.eye(d)
        R_full[:2, :2] = R_true
        
        # Generate data
        X = torch.randn(n_samples, d)
        Y = scale_true * X @ R_full + 0.1 * torch.randn(n_samples, d)
        
        # Compute restriction
        procrustes = ProcrustesMaps(allow_scaling=True)
        R_computed = procrustes.compute_restriction(X, Y, method='scaled_procrustes')
        
        # Check recovery
        Y_pred = X @ R_computed
        error = torch.norm(Y - Y_pred, 'fro') / torch.norm(Y, 'fro')
        assert error < 0.1  # Less than 10% error
    
    def test_dimension_mismatch_handling(self):
        """Test handling of dimension mismatches."""
        procrustes = ProcrustesMaps()
        
        # Test projection (high to low dim)
        X = torch.randn(100, 50)
        Y = torch.randn(100, 30)
        
        R = procrustes.compute_restriction(X, Y, method='orthogonal')
        assert R.shape == (50, 30)
        
        # Test embedding (low to high dim)
        X = torch.randn(100, 30)
        Y = torch.randn(100, 50)
        
        R = procrustes.compute_restriction(X, Y, method='orthogonal')
        assert R.shape == (30, 50)
    
    def test_orthogonality_preservation(self):
        """Test orthogonal projection preserves structure."""
        # Create data with orthogonal components
        n = 200
        U, _ = torch.linalg.qr(torch.randn(50, 50))
        X = torch.randn(n, 50) @ U
        
        # Project to lower dimension
        procrustes = ProcrustesMaps()
        R = procrustes.compute_restriction(X, X[:, :30], method='orthogonal')
        
        # Check approximate orthogonality preservation
        gram_X = X.T @ X / n
        gram_proj = R.T @ gram_X @ R
        
        # Should be approximately identity (up to scale)
        I_approx = gram_proj / torch.trace(gram_proj) * 30
        error = torch.norm(I_approx - torch.eye(30), 'fro')
        assert error < 0.5
```

### Sheaf Construction Tests
```python
# tests/phase3_sheaf/integration/test_sheaf_construction.py
import pytest
import torch
import torch.nn as nn
from neurosheaf.sheaf.construction import SheafBuilder
from neurosheaf.cka import DebiasedCKA

class TestSheafConstruction:
    """Test complete sheaf construction pipeline."""
    
    def test_end_to_end_construction(self):
        """Test building sheaf from model and activations."""
        # Create simple model
        model = nn.Sequential(
            nn.Linear(10, 20),
            nn.ReLU(),
            nn.Linear(20, 30),
            nn.ReLU(),
            nn.Linear(30, 10)
        )
        
        # Generate activations
        x = torch.randn(100, 10)
        activations = {}
        
        # Hook to capture activations
        def get_activation(name):
            def hook(module, input, output):
                activations[name] = output.detach()
            return hook
        
        for i, layer in enumerate(model):
            layer.register_forward_hook(get_activation(f'layer_{i}'))
        
        # Forward pass
        _ = model(x)
        
        # Build sheaf
        builder = SheafBuilder()
        sheaf = builder.build_sheaf(model, activations)
        
        # Validate structure
        assert len(sheaf.stalks) == len(activations)
        assert len(sheaf.restrictions) > 0
        assert sheaf.validate()
    
    def test_sheaf_with_cka_stalks(self):
        """Test using CKA matrices as stalk data."""
        # Create model
        model = nn.Sequential(
            nn.Conv2d(3, 64, 3),
            nn.ReLU(),
            nn.Conv2d(64, 128, 3),
            nn.ReLU()
        )
        
        # Generate activations
        x = torch.randn(50, 3, 32, 32)
        activations = {}
        
        for i, layer in enumerate(model):
            x = layer(x)
            if isinstance(layer, nn.ReLU):
                # Flatten spatial dimensions for CKA
                activations[f'layer_{i}'] = x.flatten(2).mean(dim=2)
        
        # Compute CKA matrices
        cka_computer = DebiasedCKA()
        cka_matrices = {}
        
        layer_names = list(activations.keys())
        for name in layer_names:
            # Compute CKA with all layers
            cka_row = []
            for other_name in layer_names:
                cka_value = cka_computer.compute(
                    activations[name],
                    activations[other_name]
                )
                cka_row.append(cka_value)
            cka_matrices[name] = torch.tensor(cka_row)
        
        # Build sheaf with CKA stalks
        builder = SheafBuilder()
        sheaf = builder.build_sheaf(model, activations, cka_matrices)
        
        # Check stalks are CKA vectors
        for name, stalk in sheaf.stalks.items():
            assert stalk.shape[0] == len(layer_names)
            assert torch.all(stalk >= 0) and torch.all(stalk <= 1)
```

### Laplacian Assembly Tests
```python
# tests/phase3_sheaf/unit/test_sparse_laplacian.py
import pytest
import torch
import numpy as np
from neurosheaf.sheaf.laplacian import SparseLaplacianBuilder
from neurosheaf.sheaf.construction import Sheaf
import networkx as nx

class TestSparseLaplacian:
    """Test sparse Laplacian construction."""
    
    def test_laplacian_properties(self):
        """Test mathematical properties of sheaf Laplacian."""
        # Create simple sheaf
        poset = nx.DiGraph()
        poset.add_edges_from([('A', 'B'), ('B', 'C')])
        
        stalks = {
            'A': torch.eye(3),
            'B': torch.eye(3),
            'C': torch.eye(3)
        }
        
        restrictions = {
            ('A', 'B'): torch.eye(3) * 0.9,
            ('B', 'C'): torch.eye(3) * 0.8,
            ('A', 'C'): torch.eye(3) * 0.72  # 0.9 * 0.8
        }
        
        sheaf = Sheaf(poset, stalks, restrictions)
        
        # Build Laplacian
        builder = SparseLaplacianBuilder(use_gpu=False)
        L = builder.build_laplacian(sheaf)
        L_dense = L.to_dense()
        
        # Check symmetry
        assert torch.allclose(L_dense, L_dense.T, atol=1e-6)
        
        # Check positive semi-definite
        eigenvalues = torch.linalg.eigvalsh(L_dense)
        assert torch.all(eigenvalues >= -1e-6)
        
        # Check row sums (approximately zero for normalized)
        row_sums = L_dense.sum(dim=1)
        assert torch.allclose(row_sums, torch.zeros_like(row_sums), atol=1e-6)
    
    def test_sparse_efficiency(self):
        """Test sparse representation saves memory."""
        # Create larger sheaf
        n_nodes = 50
        node_dim = 100
        
        poset = nx.path_graph(n_nodes, create_using=nx.DiGraph)
        
        stalks = {str(i): torch.eye(node_dim) for i in range(n_nodes)}
        restrictions = {
            (str(i), str(i+1)): torch.eye(node_dim) * 0.95
            for i in range(n_nodes - 1)
        }
        
        sheaf = Sheaf(poset, stalks, restrictions)
        
        # Build sparse Laplacian
        builder = SparseLaplacianBuilder(use_gpu=False)
        L_sparse = builder.build_laplacian(sheaf)
        
        # Check sparsity
        n_total = n_nodes * node_dim
        nnz = L_sparse._nnz()
        sparsity = nnz / (n_total ** 2)
        
        # Should be very sparse for path graph
        assert sparsity < 0.01  # Less than 1% non-zero
    
    def test_normalized_laplacian(self):
        """Test normalized Laplacian computation."""
        # Create sheaf
        poset = nx.complete_graph(4, create_using=nx.DiGraph)
        stalks = {str(i): torch.eye(2) for i in range(4)}
        restrictions = {
            (str(i), str(j)): torch.eye(2) * 0.8
            for i in range(4) for j in range(4) if i != j
        }
        
        sheaf = Sheaf(poset, stalks, restrictions)
        
        # Build normalized Laplacian
        builder = SparseLaplacianBuilder(use_gpu=False)
        L_norm = builder.build_normalized_laplacian(sheaf)
        L_norm_dense = L_norm.to_dense()
        
        # Check eigenvalues in [0, 2]
        eigenvalues = torch.linalg.eigvalsh(L_norm_dense)
        assert torch.all(eigenvalues >= -1e-6)
        assert torch.all(eigenvalues <= 2.01)
```

### Performance Tests
```python
# tests/phase3_sheaf/validation/test_performance.py
import pytest
import torch
import torch.nn as nn
import time
import psutil
from neurosheaf.sheaf import SheafBuilder
from neurosheaf.sheaf.laplacian import SparseLaplacianBuilder

class TestSheafPerformance:
    """Test performance of sheaf construction."""
    
    @pytest.mark.slow
    def test_large_network_construction(self):
        """Test sheaf construction on large networks."""
        # Create ResNet-like model
        class LargeModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.layers = nn.ModuleList()
                for i in range(50):  # 50 layers
                    self.layers.append(nn.Linear(512, 512))
                    self.layers.append(nn.ReLU())
            
            def forward(self, x):
                for layer in self.layers:
                    x = layer(x)
                return x
        
        model = LargeModel()
        
        # Generate activations
        x = torch.randn(100, 512)
        activations = {}
        
        for i, layer in enumerate(model.layers):
            x = layer(x)
            if i % 2 == 1:  # After ReLU
                activations[f'layer_{i//2}'] = x.clone()
        
        # Time construction
        builder = SheafBuilder()
        start_time = time.time()
        
        sheaf = builder.build_sheaf(model, activations)
        
        construction_time = time.time() - start_time
        
        # Should complete in reasonable time
        assert construction_time < 30  # 30 seconds
        assert len(sheaf.stalks) == 50
        assert len(sheaf.restrictions) > 0
    
    @pytest.mark.memory
    def test_memory_usage(self):
        """Test memory usage remains bounded."""
        process = psutil.Process()
        
        # Create model with many layers
        model = nn.Sequential(*[
            nn.Linear(256, 256) if i % 2 == 0 else nn.ReLU()
            for i in range(100)
        ])
        
        # Generate activations
        x = torch.randn(500, 256)
        activations = {}
        
        for i, layer in enumerate(model):
            x = layer(x)
            if isinstance(layer, nn.ReLU):
                activations[f'relu_{i}'] = x.clone()
        
        # Measure memory before
        mem_before = process.memory_info().rss / 1024 / 1024
        
        # Build sheaf
        builder = SheafBuilder()
        sheaf = builder.build_sheaf(model, activations)
        
        # Build Laplacian
        laplacian_builder = SparseLaplacianBuilder()
        L = laplacian_builder.build_laplacian(sheaf)
        
        # Measure memory after
        mem_after = process.memory_info().rss / 1024 / 1024
        memory_used = mem_after - mem_before
        
        # Should use reasonable memory
        assert memory_used < 1000  # Less than 1GB
```

## Success Criteria

1. **FX Poset Extraction**
   - Works with common architectures (ResNet, Transformer, etc.)
   - Correctly identifies skip connections
   - Graceful fallback for dynamic models
   
2. **Restriction Maps**
   - Procrustes alignment achieves <10% error
   - Handles dimension mismatches correctly
   - Numerically stable
   
3. **Sheaf Construction**
   - Validates mathematical properties
   - Supports CKA matrices as stalks
   - Efficient memory usage
   
4. **Laplacian Assembly**
   - Sparse representation saves >90% memory
   - Maintains mathematical properties
   - Fast eigenvalue computation

## Phase 3 Deliverables

1. **FX-based Poset Extraction**
   - Automatic architecture analysis
   - Skip connection detection
   - Dynamic model fallback

2. **Restriction Map Computation**
   - Scaled Procrustes implementation
   - Dimension mismatch handling
   - Multiple method options

3. **Sheaf Construction**
   - Complete sheaf data structure
   - Validation of sheaf axioms
   - Integration with CKA

4. **Sparse Laplacian**
   - Memory-efficient implementation
   - Normalized Laplacian option
   - Fast spectral methods

5. **Documentation**
   - Architecture support guide
   - Mathematical background
   - Performance tuning tips